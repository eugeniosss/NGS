DIC_FILE="dic.txt"
AR="processed_A"
REF="/media/jbod2/eugenio/ref_genomes/genome_L.tim_GCA_009760805.1_MT_euro_AJ421471.fasta"
BAMS_UN="bams_genome_mem_u"
BAMS_UN_PIC="bams_genome_mem_u_pic"
BAMS="bams_genome_mem"
BAMS_PIC="bams_genome_mem_pic"
FASTAS="fastas_MT"
STATS="stats"
NU_BED="/data/arch-ferrets/refs/IRN10000035363_Ocun_31Oct2019_primary_targets_clean_No_X_MT.bed"
MT_BED="/media/jbod2/eugenio/ref_genomes/genome_L.tim_GCA_009760805.1_MT_euro_AJ421471_MT.bed"
NGS_SCRIPTS="/media/jbod2/eugenio/NGS_scripts/"

import csv
import re

csvfile = csv.reader(open(DIC_FILE))
DIC = dict(csvfile)

DIC_MERGE=dict()

for SEQ in DIC.keys():
    SAMPLE=SEQ.split("_")[0]
    if SAMPLE in DIC_MERGE.keys():
        if SEQ not in DIC_MERGE[SAMPLE]:
            DIC_MERGE[SAMPLE]=DIC_MERGE[SAMPLE]+" "+BAMS_UN_PIC+"/"+SEQ+".bam"
    else:
        DIC_MERGE[SAMPLE]=BAMS_UN_PIC+"/"+SEQ+".bam"

wildcard_constraints:
    SAMPLE=".{6}",
    SEQ=".{20,26}"
        
rule all:
    input:
           [BAMS_UN_PIC+"/{SEQ}.bam".format(SEQ=SEQ) for SEQ in DIC.keys()],
           "metrics_libs.csv",
           [BAMS_PIC+"/{SAMPLE}.bam".format(SAMPLE=SAMPLE) for SAMPLE in DIC_MERGE.keys()],
           "metrics_samples.csv",
           [STATS+"/{SEQ}_complexity.html".format(SEQ=SEQ) for SEQ in DIC.keys()],
           STATS+"/overall_complexity.html"

### INDEX REF ###

os.path.splitext(REF)
REF_RAW=os.path.splitext(REF)[0]
REF_EXT=os.path.splitext(REF)[1]

rule generate_circualr_ref:
    input:
        REF
    output:
        REF_RAW+"_500"+REF_EXT
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    shell:	
        "circulargenerator -e 500 -i {input} -s 'MT'"

rule bwa_index:
    input:
        REF_RAW+"_500"+REF_EXT,
    output:
        idx=multiext(REF_RAW+"_500"+REF_EXT, ".amb", ".ann", ".bwt", ".pac", ".sa"),
    log:
        "bwa_index/indexing_500.log",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    params:
        extra="",
    script:
        NGS_SCRIPTS+"scripts_snake/run_bwa_index.py"

rule create_dic:
    input:
        REF_RAW+REF_EXT,
    output:
        REF_RAW+".dict",
    log:
        "bwa_index/create_diclog",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    shell:
        "picard CreateSequenceDictionary R={input} O={output} > {log} 2>&1"

###

rule adapterremoval_pe:
    input:
       sample= lambda wildcards: DIC[wildcards.SEQ].split()
    output:
        fq1=AR+"/{SEQ}.pair1.truncated.gz",                           # trimmed mate1<reads
        fq2=AR+"/{SEQ}.pair2.truncated.gz",                           # trimmed mate2 reads
        collapsed=AR+"/{SEQ}.collapsed.gz",              # overlapping mate-pairs which have been merged into a single read
        discarded=temporary(AR+"/{SEQ}.discarded.gz"),              # reads that did not pass filters
        settings=AR+"/{SEQ}.settings"                            # parameters as well as overall statistics
    log:
        AR+"/{SEQ}.log"
    params:
        adapters="",
        extra="--collapse --trimns --trimqualities --preserve5p"
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    threads: 2
    script:
        NGS_SCRIPTS+"scripts_snake/run_AdapRemovl_merge.py"

rule combine_pe:
    input:
        fq1=AR+"/{SEQ}.pair1.truncated.gz",  
        fq2=AR+"/{SEQ}.pair2.truncated.gz",  
        collapsed=AR+"/{SEQ}.collapsed.gz",  
    output:
        AR+"/{SEQ}.combined.gz"
    shell: "cat {input.fq1} {input.fq2} {input.collapsed} > {output}"

rule bwa_aln:
    input:
        fastq=AR+"/{SAMPLE}_{LA}_{DATE}.combined.gz",
        idx=multiext(REF_RAW+"_500"+REF_EXT, ".amb", ".ann", ".bwt", ".pac", ".sa"),
    output:
        "tmp_{SAMPLE}_{LA}_{DATE}/{SAMPLE}_{LA}_{DATE}.sai",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    params:
        extra="-l 1024 -o 2 -n 0.01",
    log:
        BAMS_UN+"/{SAMPLE}_{LA}_{DATE}_mapping.log",
    threads: 23
    script:
        NGS_SCRIPTS+"scripts_snake/run_bwa_aln.py"

rule bwa_sam:
    input:
        fastq=AR+"/{SAMPLE}_{LA}_{DATE}.combined.gz",
        sai="tmp_{SAMPLE}_{LA}_{DATE}/{SAMPLE}_{LA}_{DATE}.sai",
        idx=multiext(REF_RAW+"_500"+REF_EXT, ".amb", ".ann", ".bwt", ".pac", ".sa"),
    output:
        "tmp_{SAMPLE}_{LA}_{DATE}/{SAMPLE}_{LA}_{DATE}.bam",
    params:
        extra=r"-r '@RG\tID:{SAMPLE}_{LA}_{DATE}\tSM:{SAMPLE}\tLB:{SAMPLE}_{LA}'",
        sort="none",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    log:
        BAMS_UN+"/{SAMPLE}_{LA}_{DATE}_sam.log",
    script:
        NGS_SCRIPTS+"scripts_snake/run_bwa_samxe.py"

rule circularmapper:
    input:
        bam="tmp_{SAMPLE}_{LA}_{DATE}/{SAMPLE}_{LA}_{DATE}.bam",
        ref=REF  
    output:
        "tmp_{SAMPLE}_{LA}_{DATE}/{SAMPLE}_{LA}_{DATE}_realigned.bam",
    log:
        BAMS_UN+"/{SAMPLE}_{LA}_{DATE}_cir.log",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    shell: "realignsamfile -i {input.bam} -e 500 -r {input.ref} > {log} 2>&1"

rule samtools_sort_libs:
    input:
        "tmp_{SAMPLE}_{LA}_{DATE}/{SAMPLE}_{LA}_{DATE}_realigned.bam"
    output:
        BAMS_UN+"/{SAMPLE}_{LA}_{DATE}.bam"
    log:
        BAMS_UN+"/{SAMPLE}_{LA}_{DATE}_sorting.log",
    params:
        extra=" -m 4G",
    threads: 1
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    script:
        NGS_SCRIPTS+"scripts_snake/run_samtools_sort.py"

rule mark_duplicates_libs:
    input:
        bams=BAMS_UN+"/{SAMPLE}_{LA}_{DATE}.bam"
    # optional to specify a list of BAMs; this has the same effect
    # of marking duplicates on separate read groups for a sample
    # and then merging
    output:
        bam=BAMS_UN_PIC+"/{SAMPLE}_{LA}_{DATE}.bam",
        metrics=BAMS_UN_PIC+"/{SAMPLE}_{LA}_{DATE}.metrics.txt",
    conda:
       NGS_SCRIPTS+"envs/NGS.yml"
    log:
        BAMS_UN_PIC+"/{SAMPLE}_{LA}_{DATE}.log",
    params:
        extra="--REMOVE_DUPLICATES true --VALIDATION_STRINGENCY LENIENT --ASSUME_SORT_ORDER coordinate",
    # optional specification of memory usage of the JVM that snakemake will respect with global
    # resource restrictions (https://snakemake.readthedocs.io/en/latest/snakefiles/rules.html#resources)
    # and which can be used to request RAM during cluster job submission as `{resources.mem_mb}`:
    # https://snakemake.readthedocs.io/en/latest/executing/cluster.html#job-properties
    resources:
        mem_mb=11000,
    script:
        NGS_SCRIPTS+"scripts_snake/run_picard_duplicates.py"

rule samtools_index_libs:
    input:
        BAMS_UN_PIC+"/{SAMPLE}_{LA}_{DATE}.bam",
    output:
        BAMS_UN_PIC+"/{SAMPLE}_{LA}_{DATE}.bam.bai",
    log:
        BAMS_UN_PIC+"/{SAMPLE}_{LA}_{DATE}_index.log",
    params:
        extra="",  # optional params string
    threads: 1  # This value - 1 will be sent to -@
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    script:
        NGS_SCRIPTS+"scripts_snake/run_samtools_index.py"

rule get_stats_libs:
    input:
        bam=BAMS_UN_PIC+"/{SEQ}.bam",
        bai=BAMS_UN_PIC+"/{SEQ}.bam.bai",
    output:
        STATS+"/{SEQ}_basic_stats.txt",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    params:
        bed="False"
    log:
        STATS+"/{SEQ}_basic_stats.log",
    resources:
        mem_mb=1024,
    script:
        NGS_SCRIPTS+"scripts_snake/run_get_stats.py"

rule get_fastas_MT:
    input:
        bam=BAMS_UN_PIC+"/{SEQ}.bam",
        reference=REF,
        bed=MT_BED,
        bai=BAMS_UN_PIC+"/{SEQ}.bam.bai",
    output:
        FASTAS+"/{SEQ}.fasta",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    log:
        FASTAS+"/{SEQ}.log",
    params:
        extra="-l30 -q30 -Q30 -s5 -e -M",
    # optional specification of memory usage of the JVM that snakemake will respect with global
    # resource restrictions (https://snakemake.readthedocs.io/en/latest/snakefiles/rules.html#resources)
    # and which can be used to request RAM during cluster job submission as `{resources.mem_mb}`:
    # https://snakemake.readthedocs.io/en/latest/executing/cluster.html#job-properties
    resources:
        mem_mb=1024,
    script:
        NGS_SCRIPTS+"scripts_snake/run_htsbox_pileup.py"

rule get_coverage_MT:
    input:
        bam=BAMS_UN_PIC+"/{SEQ}.bam",
        fasta=REF,
        intervals=MT_BED,
        bai=BAMS_UN_PIC+"/{SEQ}.bam.bai",
        dic=REF_RAW+".dict",
    output:
        multiext(
            "stats/{SEQ}_MT",
            "",
            ".sample_interval_summary",
            ".sample_cumulative_coverage_counts",
            ".sample_cumulative_coverage_proportions",
            ".sample_interval_statistics",
            ".sample_statistics",
            ".sample_summary"),
    log:
        STATS+"/{SEQ}_coverage_MT.log",
    params:
        extra="--minMappingQuality 30",
        java_opts="",
    conda:
        NGS_SCRIPTS+"envs/gatk3.yml",
    resources:
        mem_mb=20000,
    script:
        NGS_SCRIPTS+"scripts_snake/run_gatk_coverage.py"

rule get_metrics_libs:
    input:
        stats=[STATS+"/{SEQ}_basic_stats.txt".format(SEQ=SEQ) for SEQ in DIC.keys()],
        fastas=[FASTAS+"/{SEQ}.fasta".format(SEQ=SEQ) for SEQ in DIC.keys()],
        coverages_MT=[STATS+"/{SEQ}_MT".format(SEQ=SEQ) for SEQ in DIC.keys()],
    output:
        final="metrics_libs.csv"
    params:
        tmp="tmp_libs.txt",
        codes=list(DIC.keys()),
        stats=STATS,
        settings=AR,
        pic=BAMS_UN_PIC,
        fastas=FASTAS,
        MT_coverage=STATS,
        nuclear_coverage="False",
        reads_on_target="False"
    script:
        NGS_SCRIPTS+"scripts_snake/summarize_stats.py"


rule samtools_merge_sample:
    input:
        lambda wildcards: DIC_MERGE[wildcards.SAMPLE].split()
    output:
        "tmp_{SAMPLE}/{SAMPLE}.bam",
    log:
        BAMS+"/{SAMPLE}_merging.log",
#	"tmp_{SAMPLE}/{SAMPLE}.log",
    params:
        extra="--VALIDATION_STRINGENCY LENIENT",  # optional additional parameters as string
    threads: 1
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    script:
        NGS_SCRIPTS+"scripts_snake/run_picard_MergeSamFiles.py"

rule samtools_sort_samples:
    input:
        "tmp_{SAMPLE}/{SAMPLE}.bam"
    output:
        BAMS+"/{SAMPLE}.bam",
    log:
        BAMS+"/{SAMPLE}_sorting.log",
    params:
        extra="-m 4G",
    threads: 1
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    script:
        NGS_SCRIPTS+"scripts_snake/run_samtools_sort.py"

rule mark_duplicates_samples:
    input:
        bams=BAMS+"/{SAMPLE}.bam",
    # optional to specify a list of BAMs; this has the same effect
    # of marking duplicates on separate read groups for a sample
    # and then merging
    output:
        bam=BAMS_PIC+"/{SAMPLE}.bam",
        metrics=BAMS_PIC+"/{SAMPLE}.metrics.txt",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    log:
        BAMS_PIC+"/{SAMPLE}.log",
    params:
        extra="--REMOVE_DUPLICATES true --VALIDATION_STRINGENCY LENIENT --ASSUME_SORT_ORDER coordinate",
    # optional specification of memory usage of the JVM that snakemake will respect with global
    # resource restrictions (https://snakemake.readthedocs.io/en/latest/snakefiles/rules.html#resources)
    # and which can be used to request RAM during cluster job submission as `{resources.mem_mb}`:
    # https://snakemake.readthedocs.io/en/latest/executing/cluster.html#job-properties
    resources:
        mem_mb=11000,
    script:
        NGS_SCRIPTS+"scripts_snake/run_picard_duplicates.py"

rule samtools_index_samples:
    input:
        BAMS_PIC+"/{SAMPLE}.bam",
    output:
        BAMS_PIC+"/{SAMPLE}.bam.bai",
    log:
        BAMS_PIC+"/{SAMPLE}_index.log",
    params:
        extra="",  # optional params string
    threads: 1  # This value - 1 will be sent to -@
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    script:
        NGS_SCRIPTS+"scripts_snake/run_samtools_index.py"

rule get_fastas:
    input:
        bam=BAMS_PIC+"/{SAMPLE}.bam",
        bai=BAMS_PIC+"/{SAMPLE}.bam.bai",
        reference=REF,
        bed=MT_BED
    output:
        FASTAS+"/{SAMPLE}.fasta",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    log:
        FASTAS+"/{SAMPLE}.log",
    params:
        extra="-l30 -q30 -Q30 -s5 -e -M",
    # optional specification of memory usage of the JVM that snakemake will respect with global
    # resource restrictions (https://snakemake.readthedocs.io/en/latest/snakefiles/rules.html#resources)
    # and which can be used to request RAM during cluster job submission as `{resources.mem_mb}`:
    # https://snakemake.readthedocs.io/en/latest/executing/cluster.html#job-properties
    resources:
        mem_mb=1024,
    script:
        NGS_SCRIPTS+"scripts_snake/run_htsbox_pileup.py"

rule gatk3_coverage_MT_sample:
    input:
        bam=BAMS_PIC+"/{SAMPLE}.bam",
        fasta=REF,
        intervals=MT_BED,
        bai=BAMS_PIC+"/{SAMPLE}.bam.bai",
        dic=REF_RAW+".dict",
    output:
        file="stats/{SAMPLE}_MT",
        coverage_counts="stats/{SAMPLE}_MT.sample_cumulative_coverage_counts",
        coverage_proportions="stats/{SAMPLE}_MT.sample_cumulative_coverage_proportions",
        interval_statistics="stats/{SAMPLE}_MT.sample_interval_statistics",
        interval_summary="stats/{SAMPLE}_MT.sample_interval_summary",
        statistics="stats/{SAMPLE}_MT.sample_statistics",
        summary="stats/{SAMPLE}_MT.sample_summary",
    params:
#        extra="--minMappingQuality 30 --omitDepthOutputAtEachBase --omitIntervalStatistics"
        extra="--minMappingQuality 30"
    conda:
        NGS_SCRIPTS+"envs/gatk3.yml"
    resources:
        mem_mb=20000,
    log:
        "stats/{SAMPLE}_gatk3_converage.log"
    script:
        NGS_SCRIPTS+"scripts_snake/run_gatk_coverage.py"

rule get_stats_samples:
    input:
        bam=BAMS_PIC+"/{SAMPLE}.bam",
        bai=BAMS_PIC+"/{SAMPLE}.bam.bai",
    output:
        STATS+"/{SAMPLE}_basic_stats.txt",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    params:
        bed="False"
    log:
        STATS+"/{SAMPLE}_basic_stats.log",
    resources:
        mem_mb=1024,
    script:
        NGS_SCRIPTS+"scripts_snake/run_get_stats.py"     

rule get_metrics_samples:
    input:
        stats=[STATS+"/{SAMPLE}_basic_stats.txt".format(SAMPLE=SAMPLE) for SAMPLE in DIC_MERGE.keys()],
        fastas=[FASTAS+"/{SAMPLE}.log".format(SAMPLE=SAMPLE) for SAMPLE in DIC_MERGE.keys()],
        coverages=[STATS+"/{SAMPLE}_MT".format(SAMPLE=SAMPLE) for SAMPLE in DIC_MERGE.keys()],
    output:
        final="metrics_samples.csv"
    params:
        tmp="tmp_samples.txt",
        codes=list(DIC_MERGE.keys()),
        stats=STATS,
        settings="False",
        pic=BAMS_PIC,
        fastas=FASTAS,
        MT_coverage=STATS,
        nuclear_coverage="False",
        reads_on_target="False"
    script:
        NGS_SCRIPTS+"scripts_snake/summarize_stats.py"

rule preseq_c_curve:
    input:
        BAMS_UN+"/{SEQ}.bam",
    output:
        STATS+"/{SEQ}_complexity_obs.txt",
    log:
        STATS+"/{SEQ}_complexity_obs_log.txt",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    shell:
        "preseq c_curve -B -o {output} {input} -s 100000 > {log} 2>&1" 

rule preseq_lc_extrap:
    input:
        BAMS_UN+"/{SEQ}.bam",
    output:
        STATS+"/{SEQ}_complexity_pred.txt",
    log:
        STATS+"/{SEQ}_complexity_pred_log.txt",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    shell:
        "preseq lc_extrap -B -o {output} {input} -s 100000 > {log} 2>&1"


rule merge_preseqs:
    input:
        obs=STATS+"/{SEQ}_complexity_obs.txt",
        pred=STATS+"/{SEQ}_complexity_pred.txt",
    output:
        STATS+"/{SEQ}_complexity_overall.txt"
    log:
        STATS+"/{SEQ}_complexity_merge.txt",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    shell:
        """
        pr -m -t <(awk '{{print $1"\t"$2}}' {input.pred}) <(awk '{{print $2}}' {input.obs}) | sed 's/ //g' | sed 's/\t\+/\t/g' > {output} 2> {log}
        """

rule merge_all_preseqs:
    input:
        obs=[STATS+"/{SEQ}_complexity_obs.txt".format(SEQ=SEQ) for SEQ in DIC.keys()],
        pred=[STATS+"/{SEQ}_complexity_pred.txt".format(SEQ=SEQ) for SEQ in DIC.keys()],
    output:
        STATS+"/overall_complexity.txt"
    log:
        STATS+"/overall_complexity_log.txt",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    shell:
        """
        IDS=$(basename -s _complexity_obs.txt {input.obs} | tr '\n' ' ') && \
        echo "RUN\tTOTAL_READS\tEXPECTED_DISTINCT\tdistinct_reads" > {output} && \
        for ID in $IDS; do \
            pr -m -t <(tail -n +2 stats/${{ID}}_complexity_pred.txt | awk '{{print $1"\t"$2}}') \
            <(tail -n +2 stats/${{ID}}_complexity_obs.txt | awk '{{print $2}}') | sed 's/ //g' | \
            awk -v seq="${{ID}}" '{{print seq "\t" $0}}' | sed 's/\t\+/\t/g' >> {output}; \
        done > {log} 2>&1
        """


rule create_individual_preseq_plot:
    input:
        file=STATS+"/{SEQ}_complexity_overall.txt",
        script= NGS_SCRIPTS+"scripts_snake/plot_individual_preseq.R"
    output:
        STATS+"/{SEQ}_complexity.html"
    log:
        STATS+"/{SEQ}_plotting.log"
    conda:
        NGS_SCRIPTS+"envs/plotly.yml"
    shell:
        """
        Rscript {input.script} {input.file} {output} &> {log}
        """

rule collective_individual_preseq_plot:
    input:
        file=STATS+"/overall_complexity.txt",
        script= NGS_SCRIPTS+"scripts_snake/plot_collective_preseq.R"
    output:
        STATS+"/overall_complexity.html"
    log:
        STATS+"/overall_plotting.log"
    conda:
        NGS_SCRIPTS+"envs/plotly.yml"
    shell:
        """
        Rscript {input.script} {input.file} {output} &> {log}
        """

